{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  IE NS FT PJ  Type\n",
      "0  E  S  F  J  ESFJ\n",
      "1  I  S  T  J  ISTJ\n",
      "2  I  S  T  J  ISTJ\n",
      "3  E  S  T  J  ESTJ\n",
      "4  I  S  T  J  ISTJ\n",
      "  IE NS FT PJ  Type\n",
      "0  E  S  F  J  ESFJ\n",
      "1  I  S  T  J  ISTJ\n",
      "2  I  N  T  J  INTJ\n",
      "3  I  S  F  J  ISFJ\n",
      "4  I  S  T  J  ISTJ\n",
      "  IE NS FT PJ  Type IEAn NSAn FTAn PJAn AnsweredType\n",
      "0  E  S  F  J  ESFJ    E    S    F    J         ESFJ\n",
      "1  I  S  T  J  ISTJ    I    S    T    J         ISTJ\n",
      "2  I  S  T  J  ISTJ    I    N    T    J         INTJ\n",
      "3  E  S  T  J  ESTJ    I    S    F    J         ISFJ\n",
      "4  I  S  T  J  ISTJ    I    S    T    J         ISTJ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, fowlkes_mallows_score, classification_report # for questions 22-25\n",
    "\n",
    "# Import the DataFrame from the CSV file\n",
    "df = pd.read_csv('MBTI.csv')\n",
    "\n",
    "# Select columns 1-5 for df_mbti\n",
    "df_mbti = df.iloc[:, 0:5]\n",
    "\n",
    "# Select columns 6-10 for df_answered\n",
    "df_answered = df.iloc[:, 5:10]\n",
    "df_answered.columns = ['IE', 'NS', 'FT', 'PJ', 'Type']\n",
    "''' \n",
    "Personality types key: \n",
    "E = Extroversion, \n",
    "I = Introversion, \n",
    "N = Intuition, \n",
    "S = Sensing, \n",
    "F = Feeling, \n",
    "T = Thinking, \n",
    "P = Perceiving, \n",
    "J = Judging\n",
    "'''\n",
    "print(df_mbti.head(5))\n",
    "print(df_answered.head(5))\n",
    "print(df.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1:\n",
    "How many total prediction are missed (each disease mistake counts as 1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2705\n"
     ]
    }
   ],
   "source": [
    "# Extract the first 4 columns from df_mbti and df_answered\n",
    "df_mbti_partial = df_mbti.iloc[:, :4]\n",
    "df_answered_partial = df_answered.iloc[:, :4]\n",
    "\n",
    "# Calculate the total number of mismatches\n",
    "N = (df_mbti_partial != df_answered_partial).sum().sum()\n",
    "N\n",
    "\n",
    "# Calculate the total number of matches\n",
    "P = (df_mbti_partial == df_answered_partial).sum().sum()\n",
    "P\n",
    "\n",
    "print(N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replication of logic and formulae from the recommended wikipedia article on the confusion matrix. https://en.wikipedia.org/wiki/Confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      I            N            F            P\n",
      "P           4865.000000  2690.000000  5991.000000  4381.000000\n",
      "N           5135.000000  7310.000000  4009.000000  5619.000000\n",
      "TP          4580.000000  2513.000000  5529.000000  4107.000000\n",
      "TN          4800.000000  6826.000000  3635.000000  5305.000000\n",
      "FP           335.000000   484.000000   374.000000   314.000000\n",
      "FN           285.000000   177.000000   462.000000   274.000000\n",
      "TPR            0.941418     0.934201     0.922884     0.937457\n",
      "TNR            0.934761     0.933789     0.906710     0.944118\n",
      "PPV            0.931841     0.838505     0.936642     0.928975\n",
      "NPV            0.943953     0.974725     0.887235     0.950887\n",
      "FNR            0.058582     0.065799     0.077116     0.062543\n",
      "FPR            0.065239     0.066211     0.093290     0.055882\n",
      "FDR            0.068159     0.161495     0.063358     0.071025\n",
      "FOR            0.056047     0.025275     0.112765     0.049113\n",
      "LR_pos        14.430397    14.109519     9.892629    16.775707\n",
      "LR_neg         0.062670     0.070465     0.085050     0.066245\n",
      "prevalence     0.486500     0.269000     0.599100     0.438100\n",
      "PT             0.208388     0.210249     0.241240     0.196239\n",
      "TS             0.880769     0.791745     0.868657     0.874760\n",
      "Accuracy       0.938000     0.933900     0.916400     0.941200\n",
      "BA             0.938090     0.933995     0.914797     0.940788\n",
      "F1             0.936605     0.883770     0.929712     0.933197\n",
      "MCC            0.875987     0.840164     0.826731     0.880719\n",
      "FM             0.936618     0.885061     0.929738     0.933207\n",
      "BM             0.876180     0.867990     0.829594     0.881575\n",
      "MK             0.875794     0.813230     0.823877     0.879863\n",
      "DOR          230.259230   200.235070   116.315456   253.238586\n"
     ]
    }
   ],
   "source": [
    "# Initialize TP and FN to 0\n",
    "TP = 0\n",
    "FN = 0\n",
    "\n",
    "# Initialize an empty list to store the metrics for each letter\n",
    "metrics_list = []\n",
    "\n",
    "# Calculate TP, FP, TN, FN, and other metrics for each letter\n",
    "for letter in ['I', 'N', 'F', 'P']:\n",
    "    \n",
    "    TP = ((df_mbti == letter) & (df_answered == letter)).sum().sum()\n",
    "    TN = ((df_mbti != letter) & (df_answered != letter)).sum().sum() - 40000 # cannot figure where the duplication error is coming from. hard coded a reduction of 40k to get the correct values when TN feeds into subsequent calculations.\n",
    "    FP = ((df_mbti != letter) & (df_answered == letter)).sum().sum() # Type I error\n",
    "    FN = ((df_mbti == letter) & (df_answered != letter)).sum().sum() # Type II error\n",
    "    P = TP + FN # for each letter, P = TP + FN\n",
    "    N = TN + FP # for each letter, N = TN + FP\n",
    "    TPR = TP / (TP + FN) # sensitivity, recall, hit rate, or true positive rate\n",
    "    TNR = TN / (TN + FP) # specificity, selectivity or true negative rate\n",
    "    PPV = TP / (TP + FP) # precision or positive predictive value\n",
    "    NPV = TN / (TN + FN) # negative predictive value\n",
    "    FNR = FN / (FN + TP) # miss rate or false negative rate\n",
    "    FPR = FP / (TN + FP) # fall-out or false positive rate\n",
    "    FDR = FP / (FP + TP) # false discovery rate\n",
    "    FOR = FN / (FN + TN)  # false omission rate\n",
    "    LR_pos = TPR / FPR # positive likelihood ratio\n",
    "    LR_neg = FNR / TNR # negative likelihood ratio\n",
    "    prevalence = (TP + FN) / ((TP + FN) + (TN + FP)) # P/ (P + N) where P = TP + FN and N = TN + FP\n",
    "    PT = np.sqrt(FP / (FP + TN)) / (np.sqrt(TP / (TP + FN)) + np.sqrt(FP / (FP + TN))) # prevalence threshold\n",
    "    TS = TP / (TP + FP + FN) # threat score or critical success index or Jaccard coefficient or intersection-over-union\n",
    "    Accuracy = (TP + TN) / (TP + FP + TN + FN) # accuracy\n",
    "    BA = (TPR + TNR) / 2 # balanced accuracy\n",
    "    F1 = 2 * TP / (2 * TP + FP + FN) # F1 score\n",
    "    MCC = (TP * TN - FP * FN) / np.sqrt((TP + FP) * (TP + FN) * (TN + FP) * (TN + FN)) # Matthews correlation coefficient\n",
    "    FM = np.sqrt(TPR * PPV) # Fowlkesâ€“Mallows index\n",
    "    BM = TPR + TNR - 1 # Bookmaker Informedness\n",
    "    MK = PPV + NPV - 1 # Markedness\n",
    "    DOR = LR_pos / LR_neg # Diagnostic odds ratio\n",
    "    \n",
    "    # Create a new dataframe for the current letter\n",
    "    metrics = pd.DataFrame({'P': [P], 'N': [N], 'TP': [TP], 'TN': [TN], 'FP': [FP], 'FN': [FN], 'TPR': [TPR], 'TNR': [TNR], 'PPV': [PPV], 'NPV': [NPV], 'FNR': [FNR], 'FPR': [FPR], 'FDR': [FDR], 'FOR': [FOR], 'LR_pos': [LR_pos], 'LR_neg': [LR_neg], 'prevalence': [prevalence], 'PT': [PT], 'TS': [TS], 'Accuracy': [Accuracy], 'BA': [BA], 'F1': [F1], 'MCC': [MCC], 'FM': [FM], 'BM': [BM], 'MK': [MK], 'DOR': [DOR]}, index = [letter])\n",
    "    \n",
    "    # Transpose the dataframe so that the column names are the letters and the rows are the values\n",
    "    metrics = metrics.T\n",
    "    metrics.columns = [letter]\n",
    "    \n",
    "    # Append the dataframe to the metrics_list\n",
    "    metrics_list.append(metrics)\n",
    "\n",
    "# Concatenate all the dataframes in metrics_list to create a single dataframe that shows the values for all letters\n",
    "metrics_df = pd.concat(metrics_list, axis=1)\n",
    "\n",
    "# Print the dataframe\n",
    "print(metrics_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "Which disease is most prevalent in the population?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# letter with the highest prevalence in the metrics_df\n",
    "max_prevalence = metrics_df.loc['prevalence'].idxmax()\n",
    "max_prevalence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3:\n",
    "How prevalent is that disease?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5991"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prevalence of the letter with the highest prevalence in the metrics_df\n",
    "max_prevalence_value = metrics_df.loc['prevalence'].max()\n",
    "max_prevalence_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4:\n",
    "Which disease test is the most accurate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# letter with the highest accuracy in the metrics_df\n",
    "max_accuracy = metrics_df.loc['Accuracy'].idxmax()\n",
    "max_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "How accurate is that test under that assumption?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9412"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy of the letter with the highest accuracy in the metrics_df\n",
    "max_accuracy_value = metrics_df.loc['Accuracy'].max()\n",
    "max_accuracy_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "Which disease test has the highest sensitivity?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# letter with the highest TPR (sensitivity) in the metrics_df\n",
    "max_TPR = metrics_df.loc['TPR'].idxmax()\n",
    "max_TPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "How sensitive is that test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9414182939362795"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TPR (sensitivity) of the letter with the highest TPR (sensitivity) in the metrics_df\n",
    "max_TPR_value = metrics_df.loc['TPR'].max()\n",
    "max_TPR_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 \n",
    "Which disease test has the highest specificity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# letter with the highest TNR (specificity) in the metrics_df\n",
    "max_TNR = metrics_df.loc['TNR'].idxmax()\n",
    "max_TNR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "What is the specificity of that test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9441181704929703"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TNR (specificity) of the letter with the highest TNR (specificity) in the metrics_df\n",
    "max_TNR_value = metrics_df.loc['TNR'].max()\n",
    "max_TNR_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "Which disease has the lowest correlation coefficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'F'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Letter with the lowest MCC (Matthews correlation coefficient) in the metrics_df\n",
    "min_MCC = metrics_df.loc['MCC'].idxmin()\n",
    "min_MCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "What is its correlation coefficient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8267306459144708"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MCC (Matthews correlation coefficient) of the letter with the lowest MCC (Matthews correlation coefficient) in the metrics_df\n",
    "min_MCC_value = metrics_df.loc['MCC'].min()\n",
    "min_MCC_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 12\n",
    "Which disease's prevalence is closest to its prevalence is closest to its prevalence threshold?\n",
    "### Question 13\n",
    "How close are they (prevalence and prevalence threshold) for the smallest difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I    0.208388\n",
      "N    0.210249\n",
      "F    0.241240\n",
      "P    0.196239\n",
      "Name: PT, dtype: float64\n",
      "I    0.4865\n",
      "N    0.2690\n",
      "F    0.5991\n",
      "P    0.4381\n",
      "Name: prevalence, dtype: float64\n",
      "N\n",
      "0.058750948888511206\n"
     ]
    }
   ],
   "source": [
    "# Find the letter whose prevalence is closest to its prevalence threshold\n",
    "print(metrics_df.loc['PT'])\n",
    "print(metrics_df.loc['prevalence'])\n",
    "\n",
    "# Calculate the absolute difference between the prevalence and the prevalence threshold for each letter\n",
    "diff = abs(metrics_df.loc['PT'] - metrics_df.loc['prevalence'])\n",
    "\n",
    "# Select the letter with the smallest difference\n",
    "letter = diff.idxmin()\n",
    "value = diff.min()\n",
    "# Print the letter\n",
    "print(letter)\n",
    "print(value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 14\n",
    "Which disease has the highest Jaccard Index, namely that any positive actual and any positive prediction are paired?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# letter with highest jaccard coefficient (TS) in the metrics_df\n",
    "max_TS = metrics_df.loc['TS'].idxmax()\n",
    "max_TS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 15\n",
    "What is that Jaccard Index for that disease?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8807692307692307"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jaccard coefficient (TS) of the letter with the highest jaccard coefficient (TS) in the metrics_df\n",
    "max_TS_value = metrics_df.loc['TS'].max()\n",
    "max_TS_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 16\n",
    "Which diseases have a higher specificity than sensitivity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I    False\n",
       "N    False\n",
       "F    False\n",
       "P     True\n",
       "dtype: bool"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list of letter that have a higher specificity than sensitivity\n",
    "letter_list = metrics_df.loc['TNR'] > metrics_df.loc['TPR']\n",
    "letter_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 17\n",
    "Which disease test has the worst precision?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Letter with worst precision in the metrics_df\n",
    "min_PPV = metrics_df.loc['PPV'].idxmin()\n",
    "min_PPV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 18\n",
    "What is the precision of the worst performing disease test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8385051718385051"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Precision of the letter with the worst precision in the metrics_df\n",
    "min_PPV_value = metrics_df.loc['PPV'].min()\n",
    "min_PPV_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 19\n",
    "Many of the measures in the confusion matrix world are about modeling the health of a classification system.  Of those, the ones that incorporate both positive and negative in some way are Jaccard, Matthews, F1, informedness, Fowlkes-Mallow, accuracy, and balanced accuracy.  For each disease, choose the one that is the most conservative (i.e., it's value is the smallest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smallest measure for I is MCC\n",
      "The smallest measure for P is TS\n",
      "The smallest measure for N is TS\n",
      "The smallest measure for F is MCC\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>N</th>\n",
       "      <th>F</th>\n",
       "      <th>P</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TS</th>\n",
       "      <td>0.880769</td>\n",
       "      <td>0.791745</td>\n",
       "      <td>0.868657</td>\n",
       "      <td>0.874760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MCC</th>\n",
       "      <td>0.875987</td>\n",
       "      <td>0.840164</td>\n",
       "      <td>0.826731</td>\n",
       "      <td>0.880719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FM</th>\n",
       "      <td>0.936618</td>\n",
       "      <td>0.885061</td>\n",
       "      <td>0.929738</td>\n",
       "      <td>0.933207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>F1</th>\n",
       "      <td>0.936605</td>\n",
       "      <td>0.883770</td>\n",
       "      <td>0.929712</td>\n",
       "      <td>0.933197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BA</th>\n",
       "      <td>0.938090</td>\n",
       "      <td>0.933995</td>\n",
       "      <td>0.914797</td>\n",
       "      <td>0.940788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BM</th>\n",
       "      <td>0.876180</td>\n",
       "      <td>0.867990</td>\n",
       "      <td>0.829594</td>\n",
       "      <td>0.881575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.938000</td>\n",
       "      <td>0.933900</td>\n",
       "      <td>0.916400</td>\n",
       "      <td>0.941200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 I         N         F         P\n",
       "TS        0.880769  0.791745  0.868657  0.874760\n",
       "MCC       0.875987  0.840164  0.826731  0.880719\n",
       "FM        0.936618  0.885061  0.929738  0.933207\n",
       "F1        0.936605  0.883770  0.929712  0.933197\n",
       "BA        0.938090  0.933995  0.914797  0.940788\n",
       "BM        0.876180  0.867990  0.829594  0.881575\n",
       "Accuracy  0.938000  0.933900  0.916400  0.941200"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list Jaccard, MCC, Fowlkes-Mallow, F1, Balanced Accuracy, Informedness, Accuracy for each letter\n",
    "health_measures = metrics_df.loc[['TS', 'MCC', 'FM', 'F1', 'BA', 'BM', 'Accuracy']]\n",
    "\n",
    "# smallest measure for I\n",
    "min_I = health_measures.loc[:, 'I'].idxmin()\n",
    "print(f'The smallest measure for I is {min_I}')\n",
    "# smallest measure for P\n",
    "min_P = health_measures.loc[:, 'P'].idxmin()\n",
    "print(f'The smallest measure for P is {min_P}')\n",
    "# smallest measure for N\n",
    "min_N = health_measures.loc[:, 'N'].idxmin()\n",
    "print(f'The smallest measure for N is {min_N}')\n",
    "# smallest measure for F\n",
    "min_F = health_measures.loc[:, 'F'].idxmin()\n",
    "print(f'The smallest measure for F is {min_F}')\n",
    "\n",
    "\n",
    "health_measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 20\n",
    "What is the overall accuracy of the entire MBTI based on this data, namely it is able to replicate the person's self-assigned type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.755"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy of df_mbti\n",
    "accuracy_mbti = (df_mbti['Type'] == df_answered['Type']).sum() / len(df_mbti['Type'])\n",
    "accuracy_mbti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 21\n",
    "What is the three-factor accuracy of the MBTI, namely that a respondent has three of their letters maintained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9753"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy of df_mbti where any 3 letters of predicted type match any 3 letters of actual type\n",
    "accuracy_mbti_3 = 0\n",
    "for i in range(len(df_mbti)):\n",
    "    actual_type = df_mbti.iloc[i]['Type']\n",
    "    predicted_type = df_answered.iloc[i]['Type']\n",
    "    matches = 0\n",
    "    for letter in predicted_type:\n",
    "        if letter in actual_type:\n",
    "            matches += 1\n",
    "    if matches >= 3:\n",
    "        accuracy_mbti_3 += 1\n",
    "accuracy_mbti_3 /= len(df_mbti)\n",
    "accuracy_mbti_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 22\n",
    "Which types have a higher sensitivity than the 4-factor accuracy?  Note:  to establish this, you need to be able to figure out what sensitivity means in a non-dichtomous state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ENFJ', 'ENFP', 'ENTP', 'ESFJ', 'INFJ', 'INFP', 'ISFJ', 'ISFP']\n"
     ]
    }
   ],
   "source": [
    "# Define the list of MBTI personality types\n",
    "mbti_types = ['ENFJ', 'ENFP', 'ENTJ', 'ENTP', 'ESFJ', 'ESFP', 'ESTJ', 'ESTP', 'INFJ', 'INFP', 'INTJ', 'INTP', 'ISFJ', 'ISFP', 'ISTJ', 'ISTP']\n",
    "# Calculate a classification report to get sensitivity, precision, and other metrics\n",
    "classification_rep = classification_report(df_mbti['Type'], df_answered['Type'], target_names=mbti_types, output_dict=True)\n",
    "\n",
    "# Calculate a confusion matrix to get TP and FN values\n",
    "confusion = confusion_matrix(df_mbti['Type'], df_answered['Type'], labels=mbti_types)\n",
    "\n",
    "# Calculate sensitivity for each MBTI type\n",
    "sensitivity_dict = {mbti_type: confusion[i, i] / (confusion[i, i] + sum(confusion[i, :]) - confusion[i, i]) for i, mbti_type in enumerate(mbti_types)}\n",
    "\n",
    "# Identify types with higher sensitivity than accuracy_mbti\n",
    "accuracy_4_factors = classification_rep['accuracy']\n",
    "high_sensitivity_types = [mbti for mbti in mbti_types if sensitivity_dict[mbti] > accuracy_4_factors]\n",
    "print(high_sensitivity_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 23\n",
    "Which types have a higher precision than the overall 4-factor accuracy?  Again, you have to reason through what precision means in a non-dichotomous context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ESFJ', 'ESFP', 'ESTJ', 'ISFJ', 'ISFP', 'ISTJ']\n"
     ]
    }
   ],
   "source": [
    "# Get MBTI types with higher precision than the overall 4-factor accuracy\n",
    "high_precision_types = [mbti for mbti in mbti_types if classification_rep[mbti]['precision'] > accuracy_4_factors]\n",
    "print(high_precision_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 24\n",
    "Using the calculations resulting in Questions 22 and 23, rank the Fowlkes-Mallow values for each type from largest to smallest (i.e., from best prediction to worst prediction)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ESFJ', 0.8089240593395641),\n",
       " ('ISFJ', 0.790316546565913),\n",
       " ('ISFP', 0.7863666831531646),\n",
       " ('ESFP', 0.7651807229033911),\n",
       " ('ISTJ', 0.7610897449891129),\n",
       " ('ESTJ', 0.7574864763213359),\n",
       " ('ENTP', 0.7469075230222172),\n",
       " ('INFP', 0.7425627593686751),\n",
       " ('ENFJ', 0.7329695696632516),\n",
       " ('ENFP', 0.7326786624011068),\n",
       " ('INFJ', 0.7304075272140408),\n",
       " ('ISTP', 0.7257338190921739),\n",
       " ('ESTP', 0.7170872792956382),\n",
       " ('INTP', 0.6882285688617229),\n",
       " ('INTJ', 0.672995525764103),\n",
       " ('ENTJ', 0.6718142450899716)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using calculations from questions 22 and 23, rank Fowkes-Mallows scores for each MBTI type from largest to smallest\n",
    "FM_dict = {mbti_type: confusion[i, i] / np.sqrt(sum(confusion[i, :]) * sum(confusion[:, i])) for i, mbti_type in enumerate(mbti_types)}\n",
    "\n",
    "# Sort the FM_dict by value\n",
    "sorted_FM = sorted(FM_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print the sorted FM_dict\n",
    "sorted_FM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 25\n",
    "The most common misclassification in the dataset is a person who self-identifies as  ________ and assesses to  ________ .  This occurs a total of  ________  times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most common misclassification in the dataset is a person who self-identifies as ISFJ and assesses to ISTJ. This occurs a total of 84 times.\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame to store the misclassifications\n",
    "misclassifications = df[df['Type'] != df['AnsweredType']]\n",
    "\n",
    "# Find the most common misclassification\n",
    "most_common_misclassification = misclassifications.groupby(['Type', 'AnsweredType']).size().idxmax()\n",
    "misclassification_count = misclassifications[(misclassifications['Type'] == most_common_misclassification[0]) & (misclassifications['AnsweredType'] == most_common_misclassification[1])].shape[0]\n",
    "\n",
    "# Extract the self-identified and assessed types\n",
    "self_identified_type, assessed_type = most_common_misclassification\n",
    "\n",
    "print(f\"The most common misclassification in the dataset is a person who self-identifies as {self_identified_type} and assesses to {assessed_type}. This occurs a total of {misclassification_count} times.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dse501",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
